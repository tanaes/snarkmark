import pandas as pd
import biom
from biom.util import biom_open

def combine_profiles(profiles):
    """Combines profiles for several samples into one table.

    Parameters
    ----------
    profiles : iterable((str, str))
        An iterable of tuples, where the second component is the filepath
        pointing to a profile (format: feature<tab>count), while the first
        component defines a sample name for the profile.

    Returns
    -------
    Pandas.DataFrame with rows for features, columns for samples.
    """
    samples = [pd.read_table(file, index_col=0, names=[name], comment='#')
               for name, file in profiles]
    return pd.concat(samples, axis=1).fillna(0).astype(float)

def extract_level(table, code, delim=';', dic=None):
    """Extract features at certain level from a table.

    Parameters
    ----------
    table : Pandas.DataFrame
        with rows for features, columns for samples
        fearures are Greengenes-style lineage strings
    code : str
        single-letter abbreviation of level
    delim : str (optional)
        delimiter for levels in lineage string (default: ";")
    dic : dict (optional)
        translate rank names into TaxIDs using this dictionary

    Returns
    -------
    Pandas.DataFrame
        with features replaced by individual ranks

    Raises
    ------
    ValueError
        if there are duplicated rank names at current level

    Notes
    -----
    Rank names not found in the dictionary will be dropped.
    Duplicated TaxIDs are allowed. They will be merged and their cell values
    will be summed.
    """
    df = table.copy()

    # get last (right-most) rank of lineage get index of new column
    i = len(df.columns)
    df[i] = df.index.str.split(delim).str.get(-1)

    # only keep given level with explicit name
    df = df[(df[i].str.len() > 3)
            & df[i].str.startswith('%s__' % code)
            & ~df[i].str.endswith('_noname')]

    # check for duplicated rank names
    if df[i].duplicated().any():
        raise ValueError('Duplicated taxa detected')
    df.set_index(i, inplace=True)

    # translate rank names into TaxIDs
    if dic is not None:
        df[i] = df.index.to_series().map(dic)
        df = df.groupby(2).sum()
    df.index.name = None
    return df

def pandas2biom(file_biom, table):
    """ Writes a Pandas.DataFrame into a biom file.

    Parameters
    ----------
    file_biom: str
        The filename of the BIOM file to be created.
    table: a Pandas.DataFrame
        The table that should be written as BIOM.

    Returns
    -------
    Nothing
    """
    bt = biom.Table(table.values,
                    observation_ids=list(map(str, table.index)),
                    sample_ids=table.columns)

    with biom_open(file_biom, 'w') as f:
        bt.to_hdf5(f, "example")

rule taxonomy_metaphlan2:
    """
    Runs MetaPhlan2 on a set of samples to create a joint taxonomic profile for
    input into HUMAnN2, based on the thinking that it is preferable to have a
    consistent Chocophlan reference database for the whole set of samples. This
    is especially true for shallowly sequenced samples.

    Going to do just R1 reads for now. Because of how I've split PE vs SE
    processing and naming, still will need to make a separate rule for PE.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        profile = taxonomy_dir + "{sample}/metaphlan2/{sample}.profile.txt"
    params:
        db = os.path.join(config['params']['db_dir'],
                          config['params']['metaphlan2']['db']),
        temp_dir = lambda wildcards: os.path.join(config['tmp_dir_root'],
                                                 'tmp_%s' % wildcards.sample,
                                                  'taxonomy_metaphlan2')
    threads:
        2
    log:
        taxonomy_dir + "logs/taxonomy_metaphlan2.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_metaphlan2.sample_{sample}.txt"
    conda:
        "../envs/env_humann2.yaml"
    shell:
        """
        mkdir -p {params.temp_dir}

        # get stem file path
        stem={output.profile}
        stem=${{stem%.profile.txt}}

        # merge input files
        zcat {input.forward} {input.reverse} > {params.temp_dir}/input.fastq

        # run MetaPhlAn2 to generate taxonomic profile
        metaphlan2.py {params.temp_dir}/input.fastq \
        --input_type fastq \
        --mpa_pkl {params.db}.pkl \
        --bowtie2db {params.db} \
        --nproc {threads} \
        --tmp_dir {params.temp_dir} \
        --bowtie2out {params.temp_dir}/map.tmp \
        -o {output.profile} \
        2> {log} 1>&2

        rm -r {params.temp_dir}
        """

rule taxonomy_combine_metaphlan2:
    """
    Combines per-sample MetaPhlAn2 output profiles into single OTU tables.
    """
    input:
        expand(taxonomy_dir + "{sample}/metaphlan2/{sample}.profile.txt",
               sample=samples)
    output:
        taxonomy_dir + "metaphlan2/combined_profile.biom"
    threads:
        1
    params:
        levels = config['params']['metaphlan2']['levels']
    log:
        taxonomy_dir + "logs/taxonomy_combine_metaphlan2.log"
    run:
        table = combine_profiles(zip(samples, input))
        pandas2biom(output[0], table)
        for level in params['levels'].split(','):
            pandas2biom('%s/metaphlan2/combined_profile.%s.biom'
                        % (taxonomy_dir, level),
                        extract_level(table, level[0].lower(), delim='|'))

rule taxonomy_kraken:
    """
    Runs Kraken with Bracken to construct taxonomic profiles.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        report = taxonomy_dir + "{sample}/kraken/{sample}.report.txt",
        profile = taxonomy_dir + "{sample}/kraken/{sample}.profile.txt"
    params:
        db = os.path.join(config['params']['db_dir'],
                         config['params']['kraken']['db']),
        kmers = config['params']['bracken']['kmers'],
        levels = config['params']['kraken']['levels'],
        temp_dir = lambda wildcards: os.path.join(config['tmp_dir_root'],
                                                 'tmp_%s' % wildcards.sample,
                                                 'taxonomy_kraken')
    threads:
        12
    log:
        taxonomy_dir + "logs/taxonomy_kraken.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_kraken.sample_{sample}.txt"
    conda:
        "../envs/env_taxonomy.yaml"
    shell: 
        """
        mkdir -p {params.temp_dir}

        # get stem file path
        stem={output.report}
        stem=${{stem%.report.txt}}

        # run Kraken to align reads against reference genomes
        kraken {input.forward} {input.reverse} \
        --db {params.db} \
        --paired \
        --fastq-input \
        --gzip-compressed \
        --only-classified-output \
        --threads {threads} \
        1> {params.temp_dir}/map.tmp \
        2> {log}

        # generate hierarchical report
        kraken-report {params.temp_dir}/map.tmp \
        --db {params.db} \
        1> {output.report} \
        2>> {log}

        # generate lineage to count table
        kraken-mpa-report {params.temp_dir}/map.tmp \
        --db {params.db} \
        1> {output.profile} \
        2>> {log}

        rm -r {params.temp_dir}
        """


rule taxonomy_centrifuge:
    """
    Runs Centrifuge with Bracken to construct taxonomic profiles.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        report = taxonomy_dir + "{sample}/centrifuge/{sample}.report.txt",
        profile = taxonomy_dir + "{sample}/centrifuge/{sample}.profile.txt"
    params:
        db = os.path.join(config['params']['db_dir'],
                         config['params']['centrifuge']['db']),
        kmers = config['params']['bracken']['kmers'],
        levels = config['params']['kraken']['levels'],
        temp_dir = lambda wildcards: os.path.join(config['tmp_dir_root'],
                                                 'tmp_%s' % wildcards.sample,
                                                 'taxonomy_centrifuge')
    threads:
        12
    log:
        taxonomy_dir + "logs/taxonomy_centrifuge.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_centrifuge.sample_{sample}.txt"
    conda:
        "../envs/env_taxonomy.yaml"
    shell:
        """
        mkdir -p {params.temp_dir}

        # get stem file path
        stem={output.report}
        stem=${{stem%.report.txt}}

        # run Centrifuge to align reads against reference genomes
        centrifuge \
        -1 {input.forward} \
        -2 {input.reverse} \
        -x {params.db} \
        -p {threads} \
        -S {params.temp_dir}/map.tmp \
        --report-file {output.profile} \
        2> {log} 1>&2

        # generate Kraken-style hierarchical report
        centrifuge-kreport {params.temp_dir}/map.tmp \
        -x {params.db} \
        1> {output.report} \
        2>> {log}

        rm -r {params.temp_dir}
        """

rule taxonomy:
    input:
        expand(taxonomy_dir + "{sample}/centrifuge/{sample}.report.txt",
               sample=samples),
        expand(taxonomy_dir + "{sample}/kraken/{sample}.profile.txt",
               sample=samples)#,
#        expand(taxonomy_dir + "{sample}/metaphlan2/{sample}.profile.txt",
#               sample=samples)

